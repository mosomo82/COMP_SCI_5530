{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "41S7izHwwo4N"
      },
      "outputs": [],
      "source": [
        "pip install google-generativeai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unV27nU4-ZYY",
        "outputId": "67eebe26-b2ad-4264-e849-41af63102990"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scT5oDkT55Ki"
      },
      "source": [
        "# Generate behavioral narratives on 3 datasets using LLM Gemini\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "ecUeWOimeI0T",
        "outputId": "0436253a-ec46-4bdc-d01d-a5ef0497cf20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from https://raw.githubusercontent.com/mosomo82/COMP_SCI_5530/refs/heads/main/Project_Customer_Churn/clean_data/clean_data_text_generation.csv...\n",
            "Successfully loaded 7032 total customers.\n",
            "Created a balanced sample of 150 customers.\n",
            "\n",
            "Running '3. Few-Shot Prompting (Dataset 3)' ---\n",
            "\n",
            "Starting generation of 150 behavioral narratives ...\n",
            "(1/150) Generated Behavioral Narrative for 6302-JGYRJ)\n",
            "(2/150) Generated Behavioral Narrative for 2320-JRSDE)\n",
            "(3/150) Generated Behavioral Narrative for 2332-EFBJY)\n",
            "(4/150) Generated Behavioral Narrative for 1624-WOIWJ)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-538078621.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;31m# 2. Call the API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m             \u001b[0mnarrative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateContentResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    332\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m             response = GenerativeServiceRestTransport._GenerateContent._get_response(\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_host\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m_get_response\u001b[0;34m(host, metadata, query_params, session, timeout, transcoded_request, body)\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m             response = getattr(session, method)(\n\u001b[0m\u001b[1;32m   1049\u001b[0m                 \u001b[0;34m\"{host}{uri}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \"\"\"\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/auth/transport/requests.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTimeoutGuard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0m_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LOGGER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             response = super(AuthorizedSession, self).request(\n\u001b[0m\u001b[1;32m    536\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1431\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "import requests\n",
        "import io\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from google.colab import userdata # Import userdata to access secrets\n",
        "\n",
        "# --- 1.i. CONFIGURATION ---\n",
        "\n",
        "# Access your API key securely using Colab's Secrets Manager\n",
        "# Add your API key to Colab secrets with the name 'GOOGLE_API_KEY'\n",
        "try:\n",
        "    GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY') # Gemini\n",
        "except userdata.SecretNotFoundError:\n",
        "    raise ValueError(\"API key not found in Colab secrets. Please add it as 'GOOGLE_API_KEY'.\")\n",
        "\n",
        "# GitHub URL for your clean_data.csv file\n",
        "DATA_URL = \"https://raw.githubusercontent.com/mosomo82/COMP_SCI_5530/refs/heads/main/Project_Customer_Churn/clean_data/clean_data_text_generation.csv\"\n",
        "\n",
        "# Number of customers to sample\n",
        "N_SAMPLES = 75\n",
        "\n",
        "# Output file name\n",
        "OUTPUT_FILE = \"customers_narratives.csv\"\n",
        "\n",
        "# --- 2.i. CONFIGURE GEMINI API ---\n",
        "\n",
        "try:\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "except AttributeError:\n",
        "    print(\"Error: The Google Generative AI library is not installed correctly or the API key is invalid.\")\n",
        "    exit()\n",
        "\n",
        "# Set up the model # temp for customer behavioral narrative\n",
        "generation_config={\n",
        "    \"temperature\": 0.8,\n",
        "    \"top_p\": 0.9, # reduce randon word choices\n",
        "    \"max_output_tokens\": 2048\n",
        "}\n",
        "\n",
        "safety_settings=[\n",
        "      {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "      {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "      {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "      {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "    ]\n",
        "\n",
        "model = genai.GenerativeModel(model_name=\"gemini-2.5-flash\",\n",
        "                              generation_config=generation_config,\n",
        "                              safety_settings=safety_settings)\n",
        "\n",
        "# --- 3. DYNAMIC PROMPT CREATION FUNCTION ---\n",
        "# Dataset 1\n",
        "# Chain-of-Thought Prompting\n",
        "\n",
        "def create_prompt_cot_ds1(customer_row):\n",
        "    \"\"\"Creates a tailored LLM prompt based on the customer's profile.\"\"\"\n",
        "\n",
        "    # Base profile string\n",
        "    profile = f\"\"\"\n",
        "    - CustomerID: {customer_row['customerID']}\n",
        "    - Contract: {customer_row['Contract_Encoded']}\n",
        "    - Internet Service: {customer_row['InternetService']}\n",
        "    - Online Security: {customer_row['OnlineSecurity']}\n",
        "    - Tech Support Add-on: {customer_row['TechSupport']}\n",
        "    - Monthly Charges: {customer_row['MonthlyCharges']}\n",
        "    - Payment Method: {customer_row['AutomaticPayment']}\n",
        "    \"\"\"\n",
        "\n",
        "    # Create different scenarios for \"Churn\" vs. \"No Churn\"\n",
        "    if customer_row['Churn'] == 1:\n",
        "\n",
        "        reasons = [\n",
        "            f\"frustration with high MonthlyCharges of ${customer_row['MonthlyCharges']}\",\n",
        "            f\"unreliable {customer_row['InternetService']} internet service\",\n",
        "            f\"no online protection {customer_row['OnlineSecurity']}\",\n",
        "            f\"difficult getting help {customer_row['TechSupport']},\"\n",
        "            f\"frustration with manual payment process or an issue with automatic payment {customer_row['AutomaticPayment']},\"\n",
        "            \"a billing dispute\",\n",
        "        ]\n",
        "\n",
        "        chosen_reason = random.choice(reasons)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are a senior customer behavior analyst. Based on {chosen_reason}, generate behavior narrative (3 - 4 sentences)\n",
        "        This customer is churnning (Churn = 1)\n",
        "\n",
        "        CUSTOMER PROFILE:\n",
        "        {profile}\n",
        "\n",
        "        INSTRUCTIONS:\n",
        "        Infer with customer's likely habits, personality and relationship with their services, focusing on\n",
        "\n",
        "        1. Tech-related skills and issues: (from InternetService, OnlineSecurity, TechSupport)\n",
        "        2. Financial and contractual habits: ( from Contract_Encoded, AutomaticPayment, MonthlyCharges)\n",
        "        3. Overall customer \"persona\" created:\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "    else:  # Churn == 0\n",
        "\n",
        "        reasons = [\n",
        "            f\" a simple billing question that got resolved\",\n",
        "            f\" the reliable {customer_row['InternetService']} service\",\n",
        "            f\"appreciating the included {customer_row['OnlineSecurity']} OnlineSecurity\",\n",
        "            f\" a quick technical question {customer_row['TechSupport']} that got resolved easily\",\n",
        "            f\"the convenience of automatic payments {customer_row['AutomaticPayment']}\",\n",
        "        ]\n",
        "\n",
        "        chosen_reason = f\"praising {random.choice(reasons)}\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are a senior customer behavior analyst. Based on {chosen_reason}, generate behavior narrative (3 - 4 sentences)\n",
        "        This cusotmer is not churnning.\n",
        "\n",
        "        CUSTOMER PROFILE:\n",
        "        {profile}\n",
        "\n",
        "         INSTRUCTIONS:\n",
        "        Infer with customer's likely habits, personality and relationship with their services, focusing on\n",
        "\n",
        "        1. Tech-related skills and issues: (from InternetService, OnlineSecurity, TechSupport)\n",
        "        2. Financial and contractual habits: ( from Contract_Encoded, AutomaticPayment, MonthlyCharges)\n",
        "        3. Overall customer \"persona\" created:\n",
        "\n",
        "         \"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# --- 3. DYNAMIC PROMPT CREATION FUNCTION ---\n",
        "# Dataset 2\n",
        "# # Chain-of-Thought Prompting\n",
        "\n",
        "def create_prompt_cot_ds2(customer_row):\n",
        "    \"\"\"Creates a tailored LLM prompt based on the customer's profile.\"\"\"\n",
        "\n",
        "    # Base profile string\n",
        "    profile = f\"\"\"\n",
        "    - CustomerID: {customer_row['customerID']}\n",
        "    - SeniorCitizen: {customer_row['SeniorCitizen']}\n",
        "    - Partner: {customer_row['Partner']}\n",
        "    - Dependents: {customer_row['Dependents']}\n",
        "    - MultipleLines: {customer_row['MultipleLines']}\n",
        "    - InternetService: {customer_row['InternetService']}\n",
        "    - OnlineBackup: {customer_row['OnlineBackup']}\n",
        "    - Tech Support Add-on: {customer_row['TechSupport']}\n",
        "    - Contract: {customer_row['Contract_Encoded']}\n",
        "    - Monthly Charges: {customer_row['MonthlyCharges']}\n",
        "    - TotalCharges: {customer_row['TotalCharges']}\n",
        "    \"\"\"\n",
        "    # Create different scenarios for \"Churn\" vs. \"No Churn\"\n",
        "    if customer_row['Churn'] == 1:\n",
        "\n",
        "        # Reasons are updated to use the new, more detailed variables\n",
        "        reasons = [\n",
        "            f\"frustration with high MonthlyCharges of ${customer_row['MonthlyCharges']}\",\n",
        "            f\"unreliable {customer_row['InternetService']} internet service\",\n",
        "            f\"a poor experience with TechSupport ({customer_row['TechSupport']})\",\n",
        "            f\"a billing dispute related to their MonthlCharges (${customer_row['MonthlyCharges']})\",\n",
        "            f\"feeling the high ${customer_row['MonthlyCharges']} isn't justified for their household (Partner: {customer_row['Partner']}, Dependents: {customer_row['Dependents']})\",\n",
        "            f\"feeling undervalued as a loyal customer (TotalCharges: ${customer_row['TotalCharges']})\",\n",
        "            f\"the {customer_row['Contract_Encoded']} contract ending and finding a better offer elsewhere\",\n",
        "            f\"as a SeniorCitizen ({customer_row['SeniorCitizen']}), finding the tech (like {customer_row['OnlineBackup']}) too complex for the price.\"\n",
        "        ]\n",
        "\n",
        "        chosen_reason = random.choice(reasons)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are a senior customer behavior analyst. Based on {chosen_reason}, generate a behavioral narrative (3-4 sentences) for this customer who is churning (Churn = 1).\n",
        "\n",
        "        CUSTOMER PROFILE:\n",
        "        {profile}\n",
        "\n",
        "        INSTRUCTIONS:\n",
        "        Infer the customer's likely habits, personality, and relationship with their services. Focus on:\n",
        "\n",
        "        1. **Household & Social Context:** (Analyze SeniorCitizen, Partner, Dependents)\n",
        "        2. **Tech-Savviness & Needs:** (Analyze InternetService, OnlineBackup, TechSupport, MultipleLines)\n",
        "        3. **Financial & Contractual Stance:** (Analyze Contract_Encoded, MonthlyCharges, and TotalCharges. Note if TotalCharges is high, implying long tenure.)\n",
        "        4. **Overall \"persona\" created by this data:**\n",
        "        \"\"\"\n",
        "\n",
        "    else:  # Churn == 0\n",
        "\n",
        "        # Reasons are updated to reflect loyalty and satisfaction with new variables\n",
        "        reasons = [\n",
        "            f\"a simple billing question that was resolved quickly\",\n",
        "            f\"the reliable {customer_row['InternetService']} service for their household (Partner: {customer_row['Partner']})\",\n",
        "            f\"a quick, positive experience with TechSupport ({customer_row['TechSupport']})\",\n",
        "            f\"appreciating the value of add-ons like {customer_row['OnlineBackup']}\",\n",
        "            f\"satisfaction with their {customer_row['Contract_Encoded']} contract terms\",\n",
        "            f\"feeling the service is a good value (${customer_row['MonthlyCharges']}) for their needs (Dependents: {customer_row['Dependents']})\",\n",
        "            f\"feeling valued as a loyal, long-term customer (TotalCharges: ${customer_row['TotalCharges']})\"\n",
        "        ]\n",
        "\n",
        "        chosen_reason = f\"praising {random.choice(reasons)}\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are a senior customer behavior analyst. Based on {chosen_reason}, generate a behavioral narrative (3-4 sentences) for this loyal customer who is NOT churning (Churn = 0).\n",
        "\n",
        "        CUSTOMER PROFILE:\n",
        "        {profile}\n",
        "\n",
        "         INSTRUCTIONS:\n",
        "        Infer the customer's likely habits, personality, and relationship with their services. Focus on:\n",
        "\n",
        "        1. **Household & Social Context:** (Analyze SeniorCitizen, Partner, Dependents)\n",
        "        2. **Tech-Savviness & Needs:** (Analyze InternetService, OnlineBackup, TechSupport, MultipleLines)\n",
        "        3. **Financial & Contractual Stance:** (Analyze Contract_Encoded, MonthlyCharges, and TotalCharges. Note if TotalCharges is high, implying long tenure.)\n",
        "        4. **Overall \"persona\" created by this data:**\n",
        "         \"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# --- 3. DYNAMIC PROMPT CREATION FUNCTION ---\n",
        "# Dataset 3\n",
        "# A Few-Shot Prompting\n",
        "\n",
        "FEW_SHOT_EXAMPLES = \"\"\"\n",
        "### EXAMPLE 1 (Churn = 1: The High-Cost, Low-Value Frustration)\n",
        "\n",
        "CUSTOMER PROFILE:\n",
        "- CustomerID: 5575-GNVDE\n",
        "- Dependents: No\n",
        "- InternetService: DSL\n",
        "- Tech Support Add-on: No\n",
        "- Contract: One Year\n",
        "- TotalCharges: 789.25\n",
        "\n",
        "BEHAVIORAL NARRATIVE:\n",
        "This customer, living without dependents, appears to be a moderate-use individual who tolerated the DSL service but lacked essential protections like Tech Support. Despite having a moderate contract length (One Year), their churn was triggered by the perceived low value for the cumulative cost, indicated by their sizable TotalCharges. Their final action suggests a tech-aware user who became frustrated with self-service issues and ultimately prioritized cost-effectiveness over loyalty.\n",
        "\n",
        "---\n",
        "\n",
        "### EXAMPLE 2 (Churn = 0: The Loyal, Family User)\n",
        "\n",
        "CUSTOMER PROFILE:\n",
        "- CustomerID: 3668-QPYBK\n",
        "- Dependents: Yes\n",
        "- InternetService: Fiber optic\n",
        "- Tech Support Add-on: Yes\n",
        "- Contract: Two Year\n",
        "- TotalCharges: 5425.05\n",
        "\n",
        "BEHAVIORAL NARRATIVE:\n",
        "This customer runs a high-demand household with dependents, requiring robust service features like Fiber Optic internet and dedicated Tech Support, which they readily adopted. Their significant TotalCharges and active Two Year contract highlight deep loyalty and a low-risk financial profile. The convenience and stability of the comprehensive service package ensure their satisfaction, making them a highly stable and valuable long-term asset.\n",
        "\n",
        "---\n",
        "\n",
        "### EXAMPLE 3 (Churn = 1: The Short-Term Value Seeker)\n",
        "\n",
        "CUSTOMER PROFILE:\n",
        "- CustomerID: 7590-VHIVE\n",
        "- Dependents: No\n",
        "- InternetService: No\n",
        "- Tech Support Add-on: No internet service\n",
        "- Contract: Month-to-month\n",
        "- TotalCharges: 19.95\n",
        "\n",
        "BEHAVIORAL NARRATIVE:\n",
        "A transient customer focused solely on a basic phone service with no internet, their low TotalCharges reflect a very short or minimal engagement. Without dependents, they have maximum flexibility and zero reliance on tech support. Their month-to-month contract allowed for an easy, impulsive churn decision when a cheaper alternative was found, pointing to a highly price-sensitive and non-committal persona.\n",
        "\"\"\"\n",
        "\n",
        "def create_prompt_few_shot_ds3(customer_row, examples=FEW_SHOT_EXAMPLES):\n",
        "    \"\"\"\n",
        "    Creates a tailored LLM prompt using Few-Shot In-Context Learning.\n",
        "    The few-shot examples are prepended to the new customer's data.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Base profile string for the new customer (No changes needed)\n",
        "    profile = f\"\"\"\n",
        "    - CustomerID: {customer_row['customerID']}\n",
        "    - Dependents: {customer_row['Dependents']}\n",
        "    - InternetService: {customer_row['InternetService']}\n",
        "    - Tech Support Add-on: {customer_row['TechSupport']}\n",
        "    - Contract: {customer_row['Contract_Encoded']}\n",
        "    - TotalCharges: {customer_row['TotalCharges']}\n",
        "    \"\"\"\n",
        "\n",
        "    # 2. Status Instruction for the New Customer (Simplified)\n",
        "    if customer_row['Churn'] == 1:\n",
        "        # Note: The 'reasons' list is now ONLY used to set a positive/negative tone,\n",
        "        # but the specific choice is NOT used in the final prompt instruction.\n",
        "        status_instruction = \"This customer is **churning (Churn = 1)**. Infer a comprehensive narrative detailing the likely reasons for their departure.\"\n",
        "    else:\n",
        "        status_instruction = \"This customer is **retained (Churn = 0)**. Infer a comprehensive narrative detailing their positive customer experience and reasons for loyalty.\"\n",
        "\n",
        "    # 3. Assemble the Final Few-Shot Prompt\n",
        "    prompt = f\"\"\"\n",
        "You are a senior customer behavior analyst. Your task is to generate a behavioral narrative (3-4 sentences) based on the provided customer profile.\n",
        "\n",
        "**GUIDELINES:**\n",
        "1.  **Style Match:** Strictly adopt the sophisticated, analytical style used in the examples below.\n",
        "2.  **Focus:** The narrative must synthesize the customer's household context, tech needs, contract terms and financial stance.\n",
        "3.  **Output Format:** Provide only the final BEHAVIORAL NARRATIVE text.\n",
        "\n",
        "{examples}\n",
        "\n",
        "---\n",
        "### NEW CUSTOMER TO ANALYZE\n",
        "\n",
        "{status_instruction}\n",
        "\n",
        "CUSTOMER PROFILE:\n",
        "{profile}\n",
        "\n",
        "BEHAVIORAL NARRATIVE:\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "prompt_strategies = {\n",
        "    \"1. Chain-of-Thought Prompting (Dataset 1)\": create_prompt_cot_ds1,\n",
        "    \"2. Chain-of-Thought Prompting (Dataset 2)\": create_prompt_cot_ds2,\n",
        "    \"3. Few-Shot Prompting (Dataset 3)\": create_prompt_few_shot_ds3\n",
        "}\n",
        "\n",
        "SELECTED_PROMPT_NAME = \"3. Few-Shot Prompting (Dataset 3)\" # @param [\"1. Chain-of-Thought Prompting (Dataset 1)\", \"2. Chain-of-Thought Prompting (Dataset 2)\", \"3. Few-Shot Prompting (Dataset 3)\"]\n",
        "\n",
        "print(f\"Loading data from {DATA_URL}...\")\n",
        "try:\n",
        "    response = requests.get(DATA_URL)\n",
        "    response.raise_for_status() # Raise an error for bad responses\n",
        "    df = pd.read_csv(io.StringIO(response.text))\n",
        "    print(f\"Successfully loaded {len(df)} total customers.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error: Failed to fetch data from GitHub. {e}\")\n",
        "    exit()\n",
        "\n",
        "# Create a balanced sample\n",
        "df_churn_yes = df[df['Churn'] == 1].sample(n=N_SAMPLES, random_state=42)\n",
        "df_churn_no = df[df['Churn'] == 0].sample(n=N_SAMPLES, random_state=42)\n",
        "df_sample = pd.concat([df_churn_yes, df_churn_no])\n",
        "\n",
        "print(f\"Created a balanced sample of {len(df_sample)} customers.\")\n",
        "\n",
        "# --- 5.i. GENERATE NARRATIVE ---\n",
        "prompt_function = prompt_strategies.get(SELECTED_PROMPT_NAME)\n",
        "\n",
        "print(f\"\\nRunning '{SELECTED_PROMPT_NAME}' ---\")\n",
        "\n",
        "all_narrative = []\n",
        "total = len(df_sample)\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "print(f\"\\nStarting generation of {total} behavioral narratives ...\")\n",
        "\n",
        "for index, row in df_sample.iterrows():\n",
        "    # 1. Create the dynamic prompt\n",
        "    prompt_text = prompt_function(row)\n",
        "\n",
        "    retries = 0\n",
        "    while retries < MAX_RETRIES:\n",
        "        try:\n",
        "            # 2. Call the API\n",
        "            response = model.generate_content(prompt_text)\n",
        "            narrative = response.text\n",
        "\n",
        "            # 3. Store the result\n",
        "            all_narrative.append({\n",
        "                'CustomerID': row['customerID'],\n",
        "                'Narrative': narrative,\n",
        "                'OriginalChurnStatus': row['Churn']\n",
        "            })\n",
        "\n",
        "            print(f\"({len(all_narrative)}/{total}) Generated Behavioral Narrative for {row['customerID']})\")\n",
        "            break # Break out of the retry loop on success\n",
        "\n",
        "        except Exception as e:\n",
        "            retries += 1\n",
        "            print(f\"Error generating content for {row['customerID']} on attempt {retries}. Retrying...\")\n",
        "            if retries < MAX_RETRIES:\n",
        "                time.sleep(2) # Wait a bit before retrying\n",
        "            else:\n",
        "                print(f\"Error: Failed to generate content for {row['customerID']} after {MAX_RETRIES} retries. Skipping.:{e}\")\n",
        "                all_narrative.append({\n",
        "                    'CustomerID': row['customerID'],\n",
        "                    'Narrative': f\"Error: Generation failed. {e}\",\n",
        "                    'OriginalChurnStatus': row['Churn']\n",
        "                })\n",
        "    # IMPORTANT: Add a delay to respect API rate limits (e.g., 60 requests/minute)\n",
        "    # Only sleep if we successfully generated or exhausted retries\n",
        "    if retries < MAX_RETRIES or (retries == MAX_RETRIES and len(all_narrative) < total):\n",
        "         time.sleep(1.5)\n",
        "\n",
        "# --- 6. SAVE FINAL FILE ---\n",
        "\n",
        "print(\"\\n...Generation complete.\")\n",
        "df_output = pd.DataFrame(all_narrative)\n",
        "\n",
        "# Save to CSV\n",
        "df_output.to_csv(OUTPUT_FILE, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"\\nSuccessfully created '{OUTPUT_FILE}' with {len(df_output)} entries.\")\n",
        "display(df_output.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate behavioral narratives on 3 datasets using LLM OpenAI (optional)"
      ],
      "metadata": {
        "id": "5bsB5ANBqPWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# --- 1.ii. CONFIGURATION ---\n",
        "\n",
        "try:\n",
        "    OPEN_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    client = OpenAI(api_key=OPEN_API_KEY)\n",
        "except userdata.SecretNotFoundError:\n",
        "    raise ValueError(\"API key not found in Colab secrets. Please add it as 'OPENAI_API_KEY'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: Could not initialize OpenAI client. {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# GitHub URL for your clean_data.csv file\n",
        "DATA_URL = \"https://raw.githubusercontent.com/mosomo82/COMP_SCI_5530/refs/heads/main/Project_Customer_Churn/clean_data/clean_data_text_generation.csv\"\n",
        "\n",
        "# Number of customers to sample\n",
        "N_SAMPLES = 75\n",
        "\n",
        "# --- 2.ii.  CONFIGURE OPENAI API ---\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "try:\n",
        "    client = OpenAI(api_key=OPEN_API_KEY)\n",
        "except Exception as e:\n",
        "    print(f\"Error: The OpenAI library is not installed correctly or the API key is invalid. {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 3. DYNAMIC PROMPT CREATION FUNCTION ---\n",
        "# Dataset 1\n",
        "# Chain-of-Thought Prompting\n",
        "\n",
        "def create_prompt_cot_ds1(customer_row):\n",
        "    \"\"\"Creates a tailored LLM prompt based on the customer's profile.\"\"\"\n",
        "\n",
        "    # Base profile string\n",
        "    profile = f\"\"\"\n",
        "    - CustomerID: {customer_row['customerID']}\n",
        "    - Contract: {customer_row['Contract_Encoded']}\n",
        "    - Internet Service: {customer_row['InternetService']}\n",
        "    - Online Security: {customer_row['OnlineSecurity']}\n",
        "    - Tech Support Add-on: {customer_row['TechSupport']}\n",
        "    - Monthly Charges: {customer_row['MonthlyCharges']}\n",
        "    - Payment Method: {customer_row['AutomaticPayment']}\n",
        "    \"\"\"\n",
        "\n",
        "    # Create different scenarios for \"Churn\" vs. \"No Churn\"\n",
        "    if customer_row['Churn'] == 1:\n",
        "\n",
        "        reasons = [\n",
        "            f\"frustration with high MonthlyCharges of ${customer_row['MonthlyCharges']}\",\n",
        "            f\"unreliable {customer_row['InternetService']} internet service\",\n",
        "            f\"no online protection {customer_row['OnlineSecurity']}\",\n",
        "            f\"difficult getting help {customer_row['TechSupport']},\"\n",
        "            f\"frustration with manual payment process or an issue with automatic payment {customer_row['AutomaticPayment']},\"\n",
        "            \"a billing dispute\",\n",
        "        ]\n",
        "\n",
        "        chosen_reason = random.choice(reasons)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are a senior customer behavior analyst. Based on {chosen_reason}, generate behavior narrative (3 - 4 sentences)\n",
        "        This customer is churnning (Churn = 1)\n",
        "\n",
        "        CUSTOMER PROFILE:\n",
        "        {profile}\n",
        "\n",
        "        INSTRUCTIONS:\n",
        "        Infer with customer's likely habits, personality and relationship with their services, focusing on\n",
        "\n",
        "        1. Tech-related skills and issues: (from InternetService, OnlineSecurity, TechSupport)\n",
        "        2. Financial and contractual habits: ( from Contract_Encoded, AutomaticPayment, MonthlyCharges)\n",
        "        3. Overall customer \"persona\" created:\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "    else:  # Churn == 0\n",
        "\n",
        "        reasons = [\n",
        "            f\" a simple billing question that got resolved\",\n",
        "            f\" the reliable {customer_row['InternetService']} service\",\n",
        "            f\"appreciating the included {customer_row['OnlineSecurity']} OnlineSecurity\",\n",
        "            f\" a quick technical question {customer_row['TechSupport']} that got resolved easily\",\n",
        "            f\"the convenience of automatic payments {customer_row['AutomaticPayment']}\",\n",
        "        ]\n",
        "\n",
        "        chosen_reason = f\"praising {random.choice(reasons)}\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are a senior customer behavior analyst. Based on {chosen_reason}, generate behavior narrative (3 - 4 sentences)\n",
        "        This cusotmer is not churnning.\n",
        "\n",
        "        CUSTOMER PROFILE:\n",
        "        {profile}\n",
        "\n",
        "         INSTRUCTIONS:\n",
        "        Infer with customer's likely habits, personality and relationship with their services, focusing on\n",
        "\n",
        "        1. Tech-related skills and issues: (from InternetService, OnlineSecurity, TechSupport)\n",
        "        2. Financial and contractual habits: ( from Contract_Encoded, AutomaticPayment, MonthlyCharges)\n",
        "        3. Overall customer \"persona\" created:\n",
        "\n",
        "         \"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# --- 3. DYNAMIC PROMPT CREATION FUNCTION ---\n",
        "# Dataset 2\n",
        "# # Chain-of-Thought Prompting\n",
        "\n",
        "def create_prompt_cot_ds2(customer_row):\n",
        "    \"\"\"Creates a tailored LLM prompt based on the customer's profile.\"\"\"\n",
        "\n",
        "    # Base profile string\n",
        "    profile = f\"\"\"\n",
        "    - CustomerID: {customer_row['customerID']}\n",
        "    - SeniorCitizen: {customer_row['SeniorCitizen']}\n",
        "    - Partner: {customer_row['Partner']}\n",
        "    - Dependents: {customer_row['Dependents']}\n",
        "    - MultipleLines: {customer_row['MultipleLines']}\n",
        "    - InternetService: {customer_row['InternetService']}\n",
        "    - OnlineBackup: {customer_row['OnlineBackup']}\n",
        "    - Tech Support Add-on: {customer_row['TechSupport']}\n",
        "    - Contract: {customer_row['Contract_Encoded']}\n",
        "    - Monthly Charges: {customer_row['MonthlyCharges']}\n",
        "    - TotalCharges: {customer_row['TotalCharges']}\n",
        "    \"\"\"\n",
        "    # Create different scenarios for \"Churn\" vs. \"No Churn\"\n",
        "    if customer_row['Churn'] == 1:\n",
        "\n",
        "        # Reasons are updated to use the new, more detailed variables\n",
        "        reasons = [\n",
        "            f\"frustration with high MonthlyCharges of ${customer_row['MonthlyCharges']}\",\n",
        "            f\"unreliable {customer_row['InternetService']} internet service\",\n",
        "            f\"a poor experience with TechSupport ({customer_row['TechSupport']})\",\n",
        "            f\"a billing dispute related to their MonthlCharges (${customer_row['MonthlyCharges']})\",\n",
        "            f\"feeling the high ${customer_row['MonthlyCharges']} isn't justified for their household (Partner: {customer_row['Partner']}, Dependents: {customer_row['Dependents']})\",\n",
        "            f\"feeling undervalued as a loyal customer (TotalCharges: ${customer_row['TotalCharges']})\",\n",
        "            f\"the {customer_row['Contract_Encoded']} contract ending and finding a better offer elsewhere\",\n",
        "            f\"as a SeniorCitizen ({customer_row['SeniorCitizen']}), finding the tech (like {customer_row['OnlineBackup']}) too complex for the price.\"\n",
        "        ]\n",
        "\n",
        "        chosen_reason = random.choice(reasons)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are a senior customer behavior analyst. Based on {chosen_reason}, generate a behavioral narrative (3-4 sentences) for this customer who is churning (Churn = 1).\n",
        "\n",
        "        CUSTOMER PROFILE:\n",
        "        {profile}\n",
        "\n",
        "        INSTRUCTIONS:\n",
        "        Infer the customer's likely habits, personality, and relationship with their services. Focus on:\n",
        "\n",
        "        1. **Household & Social Context:** (Analyze SeniorCitizen, Partner, Dependents)\n",
        "        2. **Tech-Savviness & Needs:** (Analyze InternetService, OnlineBackup, TechSupport, MultipleLines)\n",
        "        3. **Financial & Contractual Stance:** (Analyze Contract_Encoded, MonthlyCharges, and TotalCharges. Note if TotalCharges is high, implying long tenure.)\n",
        "        4. **Overall \"persona\" created by this data:**\n",
        "        \"\"\"\n",
        "\n",
        "    else:  # Churn == 0\n",
        "\n",
        "        # Reasons are updated to reflect loyalty and satisfaction with new variables\n",
        "        reasons = [\n",
        "            f\"a simple billing question that was resolved quickly\",\n",
        "            f\"the reliable {customer_row['InternetService']} service for their household (Partner: {customer_row['Partner']})\",\n",
        "            f\"a quick, positive experience with TechSupport ({customer_row['TechSupport']})\",\n",
        "            f\"appreciating the value of add-ons like {customer_row['OnlineBackup']}\",\n",
        "            f\"satisfaction with their {customer_row['Contract_Encoded']} contract terms\",\n",
        "            f\"feeling the service is a good value (${customer_row['MonthlyCharges']}) for their needs (Dependents: {customer_row['Dependents']})\",\n",
        "            f\"feeling valued as a loyal, long-term customer (TotalCharges: ${customer_row['TotalCharges']})\"\n",
        "        ]\n",
        "\n",
        "        chosen_reason = f\"praising {random.choice(reasons)}\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are a senior customer behavior analyst. Based on {chosen_reason}, generate a behavioral narrative (3-4 sentences) for this loyal customer who is NOT churning (Churn = 0).\n",
        "\n",
        "        CUSTOMER PROFILE:\n",
        "        {profile}\n",
        "\n",
        "         INSTRUCTIONS:\n",
        "        Infer the customer's likely habits, personality, and relationship with their services. Focus on:\n",
        "\n",
        "        1. **Household & Social Context:** (Analyze SeniorCitizen, Partner, Dependents)\n",
        "        2. **Tech-Savviness & Needs:** (Analyze InternetService, OnlineBackup, TechSupport, MultipleLines)\n",
        "        3. **Financial & Contractual Stance:** (Analyze Contract_Encoded, MonthlyCharges, and TotalCharges. Note if TotalCharges is high, implying long tenure.)\n",
        "        4. **Overall \"persona\" created by this data:**\n",
        "         \"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# --- 3. DYNAMIC PROMPT CREATION FUNCTION ---\n",
        "# Dataset 3\n",
        "# A Few-Shot Prompting\n",
        "\n",
        "FEW_SHOT_EXAMPLES = \"\"\"\n",
        "### EXAMPLE 1 (Churn = 1: The High-Cost, Low-Value Frustration)\n",
        "\n",
        "CUSTOMER PROFILE:\n",
        "- CustomerID: 5575-GNVDE\n",
        "- Dependents: No\n",
        "- InternetService: DSL\n",
        "- Tech Support Add-on: No\n",
        "- Contract: One Year\n",
        "- TotalCharges: 789.25\n",
        "\n",
        "BEHAVIORAL NARRATIVE:\n",
        "This customer, living without dependents, appears to be a moderate-use individual who tolerated the DSL service but lacked essential protections like Tech Support. Despite having a moderate contract length (One Year), their churn was triggered by the perceived low value for the cumulative cost, indicated by their sizable TotalCharges. Their final action suggests a tech-aware user who became frustrated with self-service issues and ultimately prioritized cost-effectiveness over loyalty.\n",
        "\n",
        "---\n",
        "\n",
        "### EXAMPLE 2 (Churn = 0: The Loyal, Family User)\n",
        "\n",
        "CUSTOMER PROFILE:\n",
        "- CustomerID: 3668-QPYBK\n",
        "- Dependents: Yes\n",
        "- InternetService: Fiber optic\n",
        "- Tech Support Add-on: Yes\n",
        "- Contract: Two Year\n",
        "- TotalCharges: 5425.05\n",
        "\n",
        "BEHAVIORAL NARRATIVE:\n",
        "This customer runs a high-demand household with dependents, requiring robust service features like Fiber Optic internet and dedicated Tech Support, which they readily adopted. Their significant TotalCharges and active Two Year contract highlight deep loyalty and a low-risk financial profile. The convenience and stability of the comprehensive service package ensure their satisfaction, making them a highly stable and valuable long-term asset.\n",
        "\n",
        "---\n",
        "\n",
        "### EXAMPLE 3 (Churn = 1: The Short-Term Value Seeker)\n",
        "\n",
        "CUSTOMER PROFILE:\n",
        "- CustomerID: 7590-VHIVE\n",
        "- Dependents: No\n",
        "- InternetService: No\n",
        "- Tech Support Add-on: No internet service\n",
        "- Contract: Month-to-month\n",
        "- TotalCharges: 19.95\n",
        "\n",
        "BEHAVIORAL NARRATIVE:\n",
        "A transient customer focused solely on a basic phone service with no internet, their low TotalCharges reflect a very short or minimal engagement. Without dependents, they have maximum flexibility and zero reliance on tech support. Their month-to-month contract allowed for an easy, impulsive churn decision when a cheaper alternative was found, pointing to a highly price-sensitive and non-committal persona.\n",
        "\"\"\"\n",
        "\n",
        "def create_prompt_few_shot_ds3(customer_row, examples=FEW_SHOT_EXAMPLES):\n",
        "    \"\"\"\n",
        "    Creates a tailored LLM prompt using Few-Shot In-Context Learning.\n",
        "    The few-shot examples are prepended to the new customer's data.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Base profile string for the new customer (No changes needed)\n",
        "    profile = f\"\"\"\n",
        "    - CustomerID: {customer_row['customerID']}\n",
        "    - Dependents: {customer_row['Dependents']}\n",
        "    - InternetService: {customer_row['InternetService']}\n",
        "    - Tech Support Add-on: {customer_row['TechSupport']}\n",
        "    - Contract: {customer_row['Contract_Encoded']}\n",
        "    - TotalCharges: {customer_row['TotalCharges']}\n",
        "    \"\"\"\n",
        "\n",
        "    # 2. Status Instruction for the New Customer (Simplified)\n",
        "    if customer_row['Churn'] == 1:\n",
        "        # Note: The 'reasons' list is now ONLY used to set a positive/negative tone,\n",
        "        # but the specific choice is NOT used in the final prompt instruction.\n",
        "        status_instruction = \"This customer is **churning (Churn = 1)**. Infer a comprehensive narrative detailing the likely reasons for their departure.\"\n",
        "    else:\n",
        "        status_instruction = \"This customer is **retained (Churn = 0)**. Infer a comprehensive narrative detailing their positive customer experience and reasons for loyalty.\"\n",
        "\n",
        "    # 3. Assemble the Final Few-Shot Prompt\n",
        "    prompt = f\"\"\"\n",
        "You are a senior customer behavior analyst. Your task is to generate a behavioral narrative (3-4 sentences) based on the provided customer profile.\n",
        "\n",
        "**GUIDELINES:**\n",
        "1.  **Style Match:** Strictly adopt the sophisticated, analytical style used in the examples below.\n",
        "2.  **Focus:** The narrative must synthesize the customer's household context, tech needs, contract terms and financial stance.\n",
        "3.  **Output Format:** Provide only the final BEHAVIORAL NARRATIVE text.\n",
        "\n",
        "{examples}\n",
        "\n",
        "---\n",
        "### NEW CUSTOMER TO ANALYZE\n",
        "\n",
        "{status_instruction}\n",
        "\n",
        "CUSTOMER PROFILE:\n",
        "{profile}\n",
        "\n",
        "BEHAVIORAL NARRATIVE:\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "prompt_strategies = {\n",
        "    \"1. Chain-of-Thought Prompting (Dataset 1)\": create_prompt_cot_ds1,\n",
        "    \"2. Chain-of-Thought Prompting (Dataset 2)\": create_prompt_cot_ds2,\n",
        "    \"3. Few-Shot Prompting (Dataset 3)\": create_prompt_few_shot_ds3\n",
        "}\n",
        "\n",
        "SELECTED_PROMPT_NAME = \"2. Chain-of-Thought Prompting (Dataset 2)\" # @param [\"1. Chain-of-Thought Prompting (Dataset 1)\", \"2. Chain-of-Thought Prompting (Dataset 2)\", \"3. Few-Shot Prompting (Dataset 3)\"]\n",
        "\n",
        "# --- 4. LOAD AND SAMPLE DATA ---\n",
        "\n",
        "print(f\"Loading data from {DATA_URL}...\")\n",
        "try:\n",
        "    response = requests.get(DATA_URL)\n",
        "    response.raise_for_status() # Raise an error for bad responses\n",
        "    df = pd.read_csv(io.StringIO(response.text))\n",
        "    print(f\"Successfully loaded {len(df)} total customers.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error: Failed to fetch data from GitHub. {e}\")\n",
        "    exit()\n",
        "\n",
        "# Create a balanced sample\n",
        "df_churn_yes = df[df['Churn'] == 1].sample(n=N_SAMPLES, random_state=42)\n",
        "df_churn_no = df[df['Churn'] == 0].sample(n=N_SAMPLES, random_state=42)\n",
        "df_sample = pd.concat([df_churn_yes, df_churn_no])\n",
        "\n",
        "print(f\"Created a balanced sample of {len(df_sample)} customers.\")\n",
        "\n",
        "# --- 5.ii. GENERATE NARRATIVE ---\n",
        "prompt_function = prompt_strategies.get(SELECTED_PROMPT_NAME)\n",
        "print(f\"\\nRunning '{SELECTED_PROMPT_NAME}' ---\")\n",
        "\n",
        "all_narrative = []\n",
        "total = len(df_sample)\n",
        "MAX_RETRIES = 3\n",
        "MODEL_NAME = \"gpt-4o-mini\" # Or \"gpt-3.5-turbo\"\n",
        "\n",
        "print(f\"\\nStarting generation of {total} behavioral narratives using OpenAI model: {MODEL_NAME} ...\")\n",
        "\n",
        "for index, row in df_sample.iterrows():\n",
        "    # 1. Create the dynamic prompt messages\n",
        "    prompt_messages = prompt_function(row)\n",
        "\n",
        "    retries = 0\n",
        "    while retries < MAX_RETRIES:\n",
        "        try:\n",
        "            # 2. Call the OpenAI API\n",
        "            response = client.chat.completions.create(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt_messages}],\n",
        "                temperature=0.8,      # From your original config\n",
        "                top_p=0.9,            # From your original config\n",
        "                max_tokens=2048       # From your original config (renamed)\n",
        "            )\n",
        "\n",
        "            # 3. Extract the text from the response object\n",
        "            narrative = response.choices[0].message.content.strip()\n",
        "\n",
        "            # 4. Store the result\n",
        "            all_narrative.append({\n",
        "                'CustomerID': row['customerID'],\n",
        "                'Narrative': narrative,\n",
        "                'OriginalChurnStatus': row['Churn']\n",
        "            })\n",
        "\n",
        "            print(f\"({len(all_narrative)}/{total}) Generated Behavioral Narrative for {row['customerID']})\")\n",
        "            break # Break out of the retry loop on success\n",
        "\n",
        "        except Exception as e:\n",
        "            retries += 1\n",
        "            print(f\"Error generating content for {row['customerID']} on attempt {retries}. Retrying...\")\n",
        "            if retries < MAX_RETRIES:\n",
        "                time.sleep(2) # Wait a bit before retrying\n",
        "            else:\n",
        "                print(f\"Error: Failed to generate content for {row['customerID']} after {MAX_RETRIES} retries. Skipping.:{e}\")\n",
        "                all_narrative.append({\n",
        "                    'CustomerID': row['customerID'],\n",
        "                    'Narrative': f\"Error: Generation failed. {e}\",\n",
        "                    'OriginalChurnStatus': row['Churn']\n",
        "                })\n",
        "\n",
        "    # IMPORTANT: Add a delay to respect API rate limits\n",
        "    # This is still a good idea for any API, including OpenAI's\n",
        "    if retries < MAX_RETRIES or (retries == MAX_RETRIES and len(all_narrative) < total):\n",
        "        time.sleep(1.5)\n",
        "\n",
        "\n",
        "# --- 6. SAVE FINAL FILE ---\n",
        "\n",
        "print(\"\\n...Generation complete.\")\n",
        "df_output = pd.DataFrame(all_narrative)\n",
        "\n",
        "# Save to CSV\n",
        "df_output.to_csv(OUTPUT_FILE, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"\\nSuccessfully created '{OUTPUT_FILE}' with {len(df_output)} entries.\")\n",
        "display(df_output.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "BaHBw2P5qzVB",
        "outputId": "5d1a89a2-c0bf-4540-ddd6-2c5530c20ffa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from https://raw.githubusercontent.com/mosomo82/COMP_SCI_5530/refs/heads/main/Project_Customer_Churn/clean_data/clean_data_text_generation.csv...\n",
            "Successfully loaded 7032 total customers.\n",
            "Created a balanced sample of 150 customers.\n",
            "\n",
            "Running '2. Chain-of-Thought Prompting (Dataset 2)' ---\n",
            "\n",
            "Starting generation of 150 behavioral narratives using OpenAI model: gpt-4o-mini ...\n",
            "Error generating content for 6302-JGYRJ on attempt 1. Retrying...\n",
            "Error generating content for 6302-JGYRJ on attempt 2. Retrying...\n",
            "Error generating content for 6302-JGYRJ on attempt 3. Retrying...\n",
            "Error: Failed to generate content for 6302-JGYRJ after 3 retries. Skipping.:Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-****************************************************************************************************************************************************************************************************************************************************************************************************************************LkgA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}, 'status': 401}\n",
            "Error generating content for 2320-JRSDE on attempt 1. Retrying...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1295472142.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# 2. Call the OpenAI API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m    336\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1193\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-****************************************************************************************************************************************************************************************************************************************************************************************************************************LkgA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}, 'status': 401}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1295472142.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error generating content for {row['customerID']} on attempt {retries}. Retrying...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mMAX_RETRIES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Wait a bit before retrying\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error: Failed to generate content for {row['customerID']} after {MAX_RETRIES} retries. Skipping.:{e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWt-9tO8kNsA"
      },
      "source": [
        "# Use the transformer to extract their sentiments, pain points, and satisfaction levels.\n",
        "\n",
        "**Note** Need to upload pre-generated customers_narratives (which are saved from OUTPUT_FILE.csv above) to proceed. We have provided these file to skip the time-consuming and constraints in generation process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MvCr3N4pC2E-"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-Shot Classification Approach"
      ],
      "metadata": {
        "id": "kbIdim0V6SV-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "-8Zwia5UC9tI",
        "outputId": "3d52af2c-7cd8-4507-8e2a-eb3ffe3c8978"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully logged in to Hugging Face!\n",
            "Successfully loaded n150_customers_narratives_ds1.csv\n",
            "Columns in loaded DataFrame: ['CustomerID', 'Narrative', 'OriginalChurnStatus']\n",
            "Loading classification model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "\n",
            "Analyzing narratives... This may take a moment.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2239847650.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# Loop through each row to analyze its 'Narrative'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_narrative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Narrative'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mall_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2239847650.py\u001b[0m in \u001b[0;36manalyze_narrative\u001b[0;34m(narrative)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0msent_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnarrative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mpain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnarrative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpain_point_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0msat_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnarrative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msatisfaction_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Return a dictionary with the top-scoring label for each category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/zero_shot_classification.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sequences, *args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unable to understand extra arguments {args}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis_template\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"This example is {}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChunkPipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1459\u001b[0;31m             return next(\n\u001b[0m\u001b[1;32m   1460\u001b[0m                 iter(\n\u001b[1;32m   1461\u001b[0m                     self.get_iterator(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_last\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1372\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/zero_shot_classification.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"use_cache\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_cache\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         model_outputs = {\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1601\u001b[0m             )\n\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1603\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1604\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1290\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1120\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m   1123\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_values, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    419\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_values, attention_mask, layer_head_mask, output_attentions, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "from huggingface_hub import login\n",
        "\n",
        "try:\n",
        "    # Fetch the token from secrets\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "    # Authenticate\n",
        "    login(hf_token)\n",
        "    print(\"Successfully logged in to Hugging Face!\")\n",
        "except Exception as e:\n",
        "    print(f\"Login failed: {e}\")\n",
        "\n",
        "# --- 2. Load Your Data ---\n",
        "file_name = \"n150_customers_narratives_ds1.csv\"  # The file you uploaded\n",
        "try:\n",
        "    df = pd.read_csv(file_name)\n",
        "    print(f\"Successfully loaded {file_name}\")\n",
        "    print(f\"Columns in loaded DataFrame: {df.columns.tolist()}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {file_name} was not found.\")\n",
        "    exit()\n",
        "\n",
        "# --- 3.i Set up the Zero-Shot Classification Pipeline ---\n",
        "print(\"Loading classification model...\")\n",
        "classifier = pipeline(\"zero-shot-classification\",\n",
        "                        model=\"valhalla/distilbart-mnli-12-3\")\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# --- 4. Define Your Custom Labels ---\n",
        "# Appply more descriptive phases instead of single,abstract words for better \"hypothesis\"\n",
        "sentiment_labels = ['positive sentiment', 'negative sentiment', 'mixed or no sentiment']\n",
        "\n",
        "pain_point_labels = [\n",
        "    'high price',\n",
        "    'unreliable service',\n",
        "    'poor tech support',\n",
        "    'billing dispute',\n",
        "    'complex tech',\n",
        "    'lack of value',\n",
        "    'no specific issue'\n",
        "]\n",
        "\n",
        "satisfaction_labels = [\n",
        "    'very satisfied',\n",
        "    'satisfied',\n",
        "    'neither satisfied nor dissatisfied',\n",
        "    'dissatisfied',\n",
        "    'very dissatisfied'\n",
        "]\n",
        "\n",
        "# Define the mapping based on the specialized model's output\n",
        "sentiment_map = {\n",
        "    'negative': 'negative sentiment',\n",
        "    'neutral': 'mixed or no sentiment',\n",
        "    'positive': 'positive sentiment'\n",
        "}\n",
        "\n",
        "# --- 5.i. Define the Analysis Function ---\n",
        "def analyze_narrative(narrative):\n",
        "    \"\"\"\n",
        "    Analyzes a single narrative for sentiment, pain points,\n",
        "    and satisfaction.\n",
        "    \"\"\"\n",
        "    if not isinstance(narrative, str):\n",
        "        return {'Sentiment': 'N/A', 'Pain_Point': 'N/A', 'Satisfaction': 'N/A'}\n",
        "\n",
        "    try:\n",
        "        # We set multi_label=False to force the model to pick only the single best label\n",
        "        sent_result = classifier(narrative, sentiment_labels, multi_label=False)\n",
        "        pain_result = classifier(narrative, pain_point_labels, multi_label=False)\n",
        "        sat_result = classifier(narrative, satisfaction_labels, multi_label=False)\n",
        "\n",
        "        # Return a dictionary with the top-scoring label for each category\n",
        "        return {\n",
        "            'Sentiment': sent_result['labels'][0],\n",
        "            'Pain_Point': pain_result['labels'][0],\n",
        "            'Satisfaction': sat_result['labels'][0]\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing narrative: {e}\")\n",
        "        return {'Sentiment': 'Error', 'Pain_Point': 'Error', 'Satisfaction': 'Error'}\n",
        "\n",
        "# --- 6. Apply the Function to Your DataFrame ---\n",
        "print(\"\\nAnalyzing narratives... This may take a moment.\")\n",
        "all_results= []\n",
        "\n",
        "# Loop through each row to analyze its 'Narrative'\n",
        "for index, row in df.iterrows():\n",
        "    result = analyze_narrative(row['Narrative'])\n",
        "    all_results.append(result)\n",
        "\n",
        "# Convert the list of dictionaries into a new DataFrame\n",
        "analysis_df = pd.DataFrame(all_results)\n",
        "\n",
        "# Join the new analysis columns back to your original DataFrame\n",
        "df_final = pd.concat([df, analysis_df], axis=1)\n",
        "\n",
        "# --- 7. Save and Display Results ---\n",
        "output_filename = \"narratives_with_sentiment_zeroshot.csv\"\n",
        "df_final.to_csv(output_filename, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"\\nAnalysis complete. Results saved to {output_filename}\")\n",
        "print(\"\\n--- Final Data Head with Analysis ---\")\n",
        "display(df_final.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Approach to extract sentiment, pain points and satisfaction levels"
      ],
      "metadata": {
        "id": "0UMX98ww288B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3.iii. Set up a Hybrid Approach by Specializing Text-classfication Pipeline\n",
        "# a. Set up the Sentiment Pipeline ---\n",
        "print(\"Loading specialized sentiment model...\")\n",
        "sentiment_classifier = pipeline(\"sentiment-analysis\",\n",
        "                                model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
        "\n",
        "# b. Set up the ZERO-SHOT Pipeline (for custom labels) ---\n",
        "print(\"Loading zero-shot model...\")\n",
        "zero_shot_classifier = pipeline(\"zero-shot-classification\",\n",
        "                                model=\"valhalla/distilbart-mnli-12-3\")\n",
        "                                # Or the bigger one: \"facebook/bart-large-mnli\"\n",
        "print(\"Models loaded successfully.\")\n",
        "\n",
        "# --- 4. Define Your Custom Labels ---\n",
        "# Appply more descriptive phases instead of single,abstract words for better \"hypothesis\"\n",
        "sentiment_labels = ['positive sentiment', 'negative sentiment', 'mixed or no sentiment']\n",
        "\n",
        "pain_point_labels = [\n",
        "    'high price',\n",
        "    'unreliable service',\n",
        "    'poor tech support',\n",
        "    'billing dispute',\n",
        "    'complex tech',\n",
        "    'lack of value',\n",
        "    'no specific issue'\n",
        "]\n",
        "\n",
        "satisfaction_labels = [\n",
        "    'very satisfied',\n",
        "    'satisfied',\n",
        "    'neither satisfied nor dissatisfied',\n",
        "    'dissatisfied',\n",
        "    'very dissatisfied'\n",
        "]\n",
        "\n",
        "# Define the mapping based on the specialized model's output\n",
        "sentiment_map = {\n",
        "    'negative': 'negative sentiment',\n",
        "    'neutral': 'mixed or no sentiment',\n",
        "    'positive': 'positive sentiment'\n",
        "}\n",
        "\n",
        "# --- 5.iii. Define the Analysis Function ---\n",
        "def analyze_narrative(narrative):\n",
        "    if not isinstance(narrative, str):\n",
        "        return {'Sentiment': 'N/A', 'Pain_Point': 'N/A', 'Satisfaction': 'N/A'}\n",
        "    try:\n",
        "        # Use the specialized model for sentiment\n",
        "        sent_result = sentiment_classifier(narrative)[0]\n",
        "        # This model's labels might be 'LABEL_0', 'LABEL_1', 'LABEL_2' or\n",
        "        model_sentiment = sent_result['label']\n",
        "        # Apply the manual mapping\n",
        "        sentiment = sentiment_map.get(model_sentiment, model_sentiment)\n",
        "\n",
        "        # Use the zero-shot model for your custom labels\n",
        "        pain_result = zero_shot_classifier(narrative, pain_point_labels, multi_label=False)\n",
        "        sat_result = zero_shot_classifier(narrative, satisfaction_labels, multi_label=False)\n",
        "\n",
        "        return {\n",
        "            'Sentiment': sentiment,\n",
        "            'Pain_Point': pain_result['labels'][0],\n",
        "            'Satisfaction': sat_result['labels'][0]\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing narrative: {e}\")\n",
        "        return {'Sentiment': 'Error', 'Pain_Point': 'Error', 'Satisfaction': 'Error'}\n",
        "\n",
        "# --- 6. Apply the Function to Your DataFrame ---\n",
        "print(\"\\nAnalyzing narratives... This may take a moment.\")\n",
        "all_results= []\n",
        "\n",
        "# Loop through each row to analyze its 'Narrative'\n",
        "for index, row in df.iterrows():\n",
        "    result = analyze_narrative(row['Narrative'])\n",
        "    all_results.append(result)\n",
        "\n",
        "# Convert the list of dictionaries into a new DataFrame\n",
        "analysis_df = pd.DataFrame(all_results)\n",
        "\n",
        "# Join the new analysis columns back to your original DataFrame\n",
        "df_final = pd.concat([df, analysis_df], axis=1)\n",
        "\n",
        "# --- 7. Save and Display Results ---\n",
        "output_filename = \"narratives_with_sentiment_hyprid.csv\"\n",
        "df_final.to_csv(output_filename, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"\\nAnalysis complete. Results saved to {output_filename}\")\n",
        "print(\"\\n--- Final Data Head with Analysis ---\")\n",
        "display(df_final.head())\n"
      ],
      "metadata": {
        "id": "Pu3cOvLw3N8I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1e2d88d5a46b426c9356036d2787a77e",
            "dbe0dde5382941f986f98ebc1c3a6ca7",
            "37871460e7d04d8582dbf55c1f497cb8",
            "0a3f4da58d2545898e68550222afe9ca",
            "34a831d00b224d46a4b36a50c82c19ac",
            "29600155ea044bc9adf119d582933ba0",
            "ed76a4d729be4194a595b8bb0087bb1e",
            "1d3e2f784ac04da895bcdf17ae805d27",
            "82aa60e8c6d145bd8cda09bb636af056",
            "4ff537c25fb8485eb307353e9d16bc5a",
            "a2139bb07e7b4f66b48187ce008872f1",
            "950fca61684f4dbdb21f7e666ccc8659",
            "2a8376fcd804487c9b2e81b5c3a76e35",
            "85f65d2f6ced455d80f779c82623e8bc",
            "361c7ff46c844c1ca23081f9628a2268",
            "f08fcadb1cb7422d89c102d2bac851fc",
            "e0993d60ecc349a4900fcc44ba3edbf5",
            "ab414bdb7683473a8a5f2a1c7046345b",
            "ff51d29ae79b499a9e2d144bb0692651",
            "0c336b5810bb4e708748d155be282e90",
            "7686da2d71954fe78ce40e390da5dbfe",
            "16ee83fd62e3474787f03c2849297db8",
            "fa08c359f40048f880be4070c1c4c092",
            "c09d162e509b4f28b273e531253966ae",
            "32b63c271c1b4fb7b02662ac939bb47e",
            "e7430a6a4f564418b6e7e5755e6863f9",
            "1620a74d6e5645ecb74a556cbad62905",
            "4efcc029f16a4a6095f40df6047eccd0",
            "8e8ae0d336d846418cf6b39e864ea91c",
            "ecb94cbc51d24d3dbf753dbd6ac809aa",
            "b0de5f8f1be1482fb3b6d57d64e5cdf0",
            "d84f4345b6cf4b3d8f8fd8894f73108b",
            "2a085bd9ccf94a6f935eeff845a02cee",
            "bb2754e1e74240c6941f07118f054b2f",
            "ab5e5b06e87b4208af43a120f6a1170a",
            "f2ce217c76df4a17999fafb037538377",
            "cdab124ddb8a4e528ab7f82da085334c",
            "4f6d2b8fb70049cc8e34be975c8a851e",
            "014c875a34cc42fdbcf45b66cca96d79",
            "3ecf38dde2e94b659b8ab86e015df7bd",
            "5976ac8287ea43f788182b933514c8d6",
            "8cf836afcbee4513875dfaeb098919b5",
            "fb832d61a9144f639fd2764c63b4bb70",
            "0c3a75be0255445996e35c3c30670935",
            "c7b92b3d6b634114b5ffe8cbf4c0b56d",
            "6f718d9da5614e60ba280534dd2346df",
            "b34dba7f59ac45689181a735137ec982",
            "ad4cb70a10234ee4aeab78c2d8b9c204",
            "a340f61d8f6e48b78c45b02bfbfb0dfd",
            "1e4b63cb3f754641a5dcec3857635687",
            "eadf599c6f5e4dc3b56436a414b3f400",
            "b4ea77aaccc74b629c4e7ab8ebe13dcd",
            "069d073cc86649888c72e0fb06b13724",
            "1760032586794ce6ba155a845f8996b6",
            "70dae7891adc43f584e78873173eeac3",
            "6ff81d4ebeb84ec992e10aa4c7a5f76f",
            "92afddbcefa849e6897395328e820792",
            "418243db23804f52bd98fa431c0593c7",
            "2bd90de0efc54f5689d137f2861d5213",
            "a56a560ce76740d6977b1b2fcbfeef19",
            "10c585d3f0bd46ff8f5dcdeac87127b6",
            "0c36920b1509414989f7da60b57a2961",
            "7e103c36074d49358e6f215a2b885e10",
            "3557fb51f7c240e78431cfba5f53cac0",
            "77898240558b41f6965e2714e5f2607f",
            "6016029891d64ade9589f3c67b06c171"
          ]
        },
        "outputId": "bcb359b9-c2e8-4870-dded-488861bf4030"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading specialized sentiment model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e2d88d5a46b426c9356036d2787a77e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "950fca61684f4dbdb21f7e666ccc8659"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa08c359f40048f880be4070c1c4c092"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb2754e1e74240c6941f07118f054b2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7b92b3d6b634114b5ffe8cbf4c0b56d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ff81d4ebeb84ec992e10aa4c7a5f76f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading zero-shot model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models loaded successfully.\n",
            "\n",
            "Analyzing narratives... This may take a moment.\n",
            "\n",
            "Analysis complete. Results saved to narratives_with_sentiment_hyprid.csv\n",
            "\n",
            "--- Final Data Head with Analysis ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   CustomerID                                          Narrative  \\\n",
              "0  6302-JGYRJ  This customer, likely less tech-proficient, co...   \n",
              "1  2320-JRSDE  This customer exhibits a low-engagement profil...   \n",
              "2  2332-EFBJY  This customer primarily uses basic, non-intern...   \n",
              "3  1624-WOIWJ  This customer, a discerning user of high-speed...   \n",
              "4  9391-EOYLI  This customer values high-speed fiber optic in...   \n",
              "\n",
              "   OriginalChurnStatus              Sentiment     Pain_Point  \\\n",
              "0                    1     negative sentiment     high price   \n",
              "1                    1  mixed or no sentiment  lack of value   \n",
              "2                    1  mixed or no sentiment  lack of value   \n",
              "3                    1  mixed or no sentiment     high price   \n",
              "4                    1  mixed or no sentiment     high price   \n",
              "\n",
              "        Satisfaction  \n",
              "0  very dissatisfied  \n",
              "1       dissatisfied  \n",
              "2       dissatisfied  \n",
              "3       dissatisfied  \n",
              "4       dissatisfied  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7e6b993d-62b6-4514-8314-f8989cabf15e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CustomerID</th>\n",
              "      <th>Narrative</th>\n",
              "      <th>OriginalChurnStatus</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Pain_Point</th>\n",
              "      <th>Satisfaction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6302-JGYRJ</td>\n",
              "      <td>This customer, likely less tech-proficient, co...</td>\n",
              "      <td>1</td>\n",
              "      <td>negative sentiment</td>\n",
              "      <td>high price</td>\n",
              "      <td>very dissatisfied</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2320-JRSDE</td>\n",
              "      <td>This customer exhibits a low-engagement profil...</td>\n",
              "      <td>1</td>\n",
              "      <td>mixed or no sentiment</td>\n",
              "      <td>lack of value</td>\n",
              "      <td>dissatisfied</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2332-EFBJY</td>\n",
              "      <td>This customer primarily uses basic, non-intern...</td>\n",
              "      <td>1</td>\n",
              "      <td>mixed or no sentiment</td>\n",
              "      <td>lack of value</td>\n",
              "      <td>dissatisfied</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1624-WOIWJ</td>\n",
              "      <td>This customer, a discerning user of high-speed...</td>\n",
              "      <td>1</td>\n",
              "      <td>mixed or no sentiment</td>\n",
              "      <td>high price</td>\n",
              "      <td>dissatisfied</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9391-EOYLI</td>\n",
              "      <td>This customer values high-speed fiber optic in...</td>\n",
              "      <td>1</td>\n",
              "      <td>mixed or no sentiment</td>\n",
              "      <td>high price</td>\n",
              "      <td>dissatisfied</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e6b993d-62b6-4514-8314-f8989cabf15e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7e6b993d-62b6-4514-8314-f8989cabf15e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7e6b993d-62b6-4514-8314-f8989cabf15e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-45c4b4de-f436-41ce-a13d-3e1931a5beba\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-45c4b4de-f436-41ce-a13d-3e1931a5beba')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-45c4b4de-f436-41ce-a13d-3e1931a5beba button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df_final\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"CustomerID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2320-JRSDE\",\n          \"9391-EOYLI\",\n          \"2332-EFBJY\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Narrative\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"This customer exhibits a low-engagement profile, subscribing only to basic services and notably opting out of internet, online security, and tech support add-ons, suggesting minimal tech reliance or fulfilling those needs through alternative providers. Their month-to-month contract and low $19.9 monthly charge highlight a budget-conscious approach, prioritizing flexibility over bundled services or long-term commitments. Coupled with a manual payment method, this customer maintains a detached relationship with the provider. Their churn is likely influenced by a small cost saving elsewhere or a decision to simply discontinue a non-essential basic service, given their minimal investment and lack of commitment.\",\n          \"This customer values high-speed fiber optic internet but opts out of additional security or tech support, suggesting either self-reliance or a desire to manage costs independently. They prefer the flexibility of a month-to-month contract and, despite having no issues with payment processing or getting help, are particularly sensitive to financial discrepancies, as evidenced by a significant billing dispute. This financially savvy individual expects transparent and accurate charges, and their decision to churn highlights that a billing dispute, not service or support issues, was the primary trigger for their departure.\",\n          \"This customer primarily uses basic, non-internet services, indicating they are not technologically reliant on this provider. Their month-to-month contract and low monthly charges suggest a cautious approach and low commitment, making them sensitive to service friction. Significant frustration over persistent payment issues, whether due to a manual process or automatic payment failures, combined with an unresolved billing dispute and difficulty getting help, has severely impacted their experience. This individual values hassle-free transactions and clear support, and the accumulated administrative challenges have directly led to their decision to churn.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"OriginalChurnStatus\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"mixed or no sentiment\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pain_Point\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"lack of value\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Satisfaction\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"dissatisfied\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For visualization purpose only, it should run individually  for each approach"
      ],
      "metadata": {
        "id": "yS1NDloP5yyB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b1fp6nVGuS5"
      },
      "outputs": [],
      "source": [
        "import altair as alt\n",
        "\n",
        "# ---8. Analyze the narratives with sentiment, satisfaction and pain points\n",
        "\n",
        "# Create a copy for plotting\n",
        "df_plot = df_final.copy()\n",
        "\n",
        "# define y-axis\n",
        "y_padding = 70\n",
        "\n",
        "# Map OriginalChurnStatus to readable labels for plotting\n",
        "df_plot['ChurnStatus_Label'] = df_plot['OriginalChurnStatus'].map({\n",
        "    0: 'Not Churned',\n",
        "    1: 'Churned'\n",
        "})\n",
        "\n",
        "# --- 8.1. Distribution Charts ---\n",
        "print(\"Generating distribution charts...\")\n",
        "\n",
        "# Chart 1: Distribution of Customer Sentiment\n",
        "chart_sentiment = alt.Chart(df_plot).mark_bar().encode(\n",
        "    x=alt.X('Sentiment', sort='-y'),\n",
        "    y=alt.Y('count()', title='Total Number of Sentiment Levels',\n",
        "            scale=alt.Scale(domain=(0, y_padding))),\n",
        "    tooltip=['Sentiment', 'count()']\n",
        ").properties(\n",
        "    title='Distribution of Customer Sentiment',\n",
        "    width=600,\n",
        "    height=400\n",
        ").interactive()\n",
        "chart_sentiment.display()\n",
        "\n",
        "# Chart 2: Distribution of Customer Pain Points\n",
        "chart_pain_point = alt.Chart(df_plot).mark_bar().encode(\n",
        "    x=alt.X('Pain_Point', sort='-y'),\n",
        "    y=alt.Y('count()', title='Total Number of Pain Points'),\n",
        "    tooltip=['Pain_Point', 'count()']\n",
        ").properties(\n",
        "    title='Distribution of Customer Pain Points',\n",
        "    width=600,\n",
        "    height=400\n",
        ").interactive()\n",
        "chart_pain_point.display()\n",
        "\n",
        "# Chart 3: Distribution of Customer Satisfaction\n",
        "chart_satisfaction = alt.Chart(df_plot).mark_bar().encode(\n",
        "    x=alt.X('Satisfaction', sort='-y'),\n",
        "    y=alt.Y('count()', title='Total Number of Satisfaction Level'),\n",
        "    tooltip=['Satisfaction', 'count()']\n",
        "   ).properties(\n",
        "    title='Distribution of Customer Satisfaction',\n",
        "    width=600,\n",
        "    height=400\n",
        ").interactive()\n",
        "chart_satisfaction.display()\n",
        "\n",
        "# --- 8.2. Relationship Charts ---\n",
        "print(\"Generating relationship charts...\")\n",
        "\n",
        "# Chart 4: Customer Satisfaction by Pain Point\n",
        "chart_pain_satisfaction = alt.Chart(df_plot).mark_bar().encode(\n",
        "    x=alt.X('Pain_Point', sort='-y'),\n",
        "    y=alt.Y('count()', title='Total Number of Pain Points'),\n",
        "    color=alt.Color('Satisfaction', sort=['very satisfied', 'satisfied', 'neutral', 'dissatisfied', 'very dissatisfied']), # Define explicit sort order\n",
        "    tooltip=['Pain_Point', 'Satisfaction', 'count()']\n",
        ").properties(\n",
        "    title='Customer Satisfaction by Pain Point',\n",
        "    width=600,\n",
        "    height=400\n",
        ").interactive()\n",
        "chart_pain_satisfaction.display()\n",
        "\n",
        "# Chart 5: Customer Churn by Pain Point\n",
        "chart_pain_churn = alt.Chart(df_plot).mark_bar().encode(\n",
        "    x=alt.X('Pain_Point', sort='-y'),\n",
        "    y=alt.Y('count()', title='Total Number of Pain Points'),\n",
        "    color='ChurnStatus_Label',\n",
        "    tooltip=['Pain_Point', 'ChurnStatus_Label', 'count()']\n",
        ").properties(\n",
        "    title='Customer Churn by Pain Point',\n",
        "    width=600,\n",
        "    height=400\n",
        ").interactive()\n",
        "chart_pain_churn.display()\n",
        "\n",
        "# Chart 6: Customer Satisfaction by Sentiment\n",
        "chart_sentiment_satisfaction = alt.Chart(df_plot).mark_bar().encode(\n",
        "    x=alt.X('Sentiment', sort='-y'),  #\n",
        "    y=alt.Y('count()', title='The total number of Sentiment Levels',\n",
        "            scale=alt.Scale(domain=(0, y_padding))),\n",
        "    color=alt.Color('Satisfaction', sort=['very satisfied', 'satisfied', 'neutral', 'dissatisfied', 'very dissatisfied']), # Define explicit sort order\n",
        "    tooltip=['Sentiment', 'Satisfaction', 'count()']\n",
        ").properties(\n",
        "    title='Customer Satisfaction by Sentiment',\n",
        "    width=600,\n",
        "    height=400\n",
        ").interactive()\n",
        "chart_sentiment_satisfaction.display()\n",
        "\n",
        "# Chart 7: Customer Churn By Sentiment\n",
        "chart_sentiment_churn = alt.Chart(df_plot).mark_bar().encode(\n",
        "    x=alt.X('Sentiment', sort='-y'),\n",
        "    y=alt.Y('count()', title='The total number of Sentiment Levels',\n",
        "            scale=alt.Scale(domain=(0, y_padding))),\n",
        "    color='ChurnStatus_Label',  # Use the mapped label\n",
        "    tooltip=['Sentiment', 'ChurnStatus_Label', 'count()']\n",
        ").properties(\n",
        "    title='Customer Churn By Sentiment',\n",
        "    width=600,\n",
        "    height=400\n",
        ").interactive()\n",
        "chart_sentiment_churn.display()\n",
        "\n",
        "# Chart 8: A heatmap of churn rate by pain points and satisfaction\n",
        "print(\"\\n Generating heatmap of churn rate by pain points and satisfaction...\")\n",
        "\n",
        "# Group by Pain_Point and Satisfaction, get total size of each group, get total churned\n",
        "df_agg = df_final.groupby(['Pain_Point', 'Satisfaction']).agg(\n",
        "    Total_Customers=('CustomerID', 'size'),\n",
        "    Churned_Customers=('OriginalChurnStatus', 'sum')\n",
        ").reset_index()\n",
        "\n",
        "# Calculate Churn Rate percentage\n",
        "df_agg['Churn_Rate_Percent'] = (df_agg['Churned_Customers'] / df_agg['Total_Customers']) * 100\n",
        "\n",
        "# Format a text label for the chart and use'.0f' to show whole numbers\n",
        "df_agg['Churn_Rate_Label'] = df_agg['Churn_Rate_Percent'].apply(lambda x: f'{x:.0f}%')\n",
        "\n",
        "print(\"Aggregation complete. Data to plot:\")\n",
        "print(df_agg.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# The base chart, using our aggregated data\n",
        "base = alt.Chart(df_agg).encode(\n",
        "    # X-axis for Satisfaction\n",
        "    x=alt.X('Satisfaction',\n",
        "            title='Customer Satisfaction',\n",
        "            axis=alt.Axis(labelAngle=0)),\n",
        "\n",
        "    # Y-axis for Pain Point\n",
        "    y=alt.Y('Pain_Point', title='Customer Pain Point'),\n",
        "\n",
        "    # Tooltip to show details on hover\n",
        "    tooltip=[\n",
        "        alt.Tooltip('Pain_Point', title='Pain Point'),\n",
        "        'Satisfaction',\n",
        "        'Total_Customers',\n",
        "        'Churned_Customers',\n",
        "        alt.Tooltip('Churn_Rate_Percent', title='Churn Rate', format='.0f')\n",
        "    ]\n",
        ").properties(\n",
        "        title='Heatmap of Churn Rate by Pain Point and Satisfaction',\n",
        "        width=600,\n",
        "        height=400\n",
        ")\n",
        "\n",
        "\n",
        "# The heatmap layer (the colored rectangles)\n",
        "heatmap = base.mark_rect().encode(\n",
        "    # Color is based on the churn rate\n",
        "    color=alt.Color('Churn_Rate_Percent',\n",
        "                    title='Churn Rate (%)',\n",
        "                    # We set a domain of 0-100 so 100% is always the \"hottest\"\n",
        "                    # 'heatmap' is a good perceptually uniform color scale\n",
        "                    scale=alt.Scale(range='heatmap', domain=[0, 100]),\n",
        "                    legend=alt.Legend(direction='vertical')\n",
        "                    )\n",
        ")\n",
        "\n",
        "# The text layer to show the percentage on each block\n",
        "text = base.mark_text(baseline='middle').encode(\n",
        "    text=alt.Text('Churn_Rate_Label'),\n",
        "\n",
        "    # If the churn rate is over 50 (darker color), use white text\n",
        "    # Otherwise (lighter color), use black text for readability\n",
        "    color=alt.condition(\n",
        "        alt.datum.Churn_Rate_Percent > 50,\n",
        "        alt.value('white'),\n",
        "        alt.value('black')\n",
        "    )\n",
        ")\n",
        "\n",
        "# Combine the heatmap and text layers\n",
        "final_chart = (heatmap + text).properties(\n",
        "    title='Heatmap of Churn Rate by Pain Point and Satisfaction'\n",
        ").interactive()\n",
        "final_chart.display()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
          "dbe0dde5382941f986f98ebc1c3a6ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29600155ea044bc9adf119d582933ba0",
            "placeholder": "",
            "style": "IPY_MODEL_ed76a4d729be4194a595b8bb0087bb1e",
            "value": "config.json:100%"
          }
        },
        "37871460e7d04d8582dbf55c1f497cb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d3e2f784ac04da895bcdf17ae805d27",
            "max": 929,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_82aa60e8c6d145bd8cda09bb636af056",
            "value": 929
          }
        },
        "0a3f4da58d2545898e68550222afe9ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ff537c25fb8485eb307353e9d16bc5a",
            "placeholder": "",
            "style": "IPY_MODEL_a2139bb07e7b4f66b48187ce008872f1",
            "value": "929/929[00:00&lt;00:00,115kB/s]"
          }
        },
        "34a831d00b224d46a4b36a50c82c19ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29600155ea044bc9adf119d582933ba0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed76a4d729be4194a595b8bb0087bb1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d3e2f784ac04da895bcdf17ae805d27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82aa60e8c6d145bd8cda09bb636af056": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ff537c25fb8485eb307353e9d16bc5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2139bb07e7b4f66b48187ce008872f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "950fca61684f4dbdb21f7e666ccc8659": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a8376fcd804487c9b2e81b5c3a76e35",
              "IPY_MODEL_85f65d2f6ced455d80f779c82623e8bc",
              "IPY_MODEL_361c7ff46c844c1ca23081f9628a2268"
            ],
            "layout": "IPY_MODEL_f08fcadb1cb7422d89c102d2bac851fc"
          }
        },
        "2a8376fcd804487c9b2e81b5c3a76e35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0993d60ecc349a4900fcc44ba3edbf5",
            "placeholder": "",
            "style": "IPY_MODEL_ab414bdb7683473a8a5f2a1c7046345b",
            "value": "pytorch_model.bin:100%"
          }
        },
        "85f65d2f6ced455d80f779c82623e8bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff51d29ae79b499a9e2d144bb0692651",
            "max": 501045531,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c336b5810bb4e708748d155be282e90",
            "value": 501045531
          }
        },
        "361c7ff46c844c1ca23081f9628a2268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7686da2d71954fe78ce40e390da5dbfe",
            "placeholder": "",
            "style": "IPY_MODEL_16ee83fd62e3474787f03c2849297db8",
            "value": "501M/501M[00:03&lt;00:00,156MB/s]"
          }
        },
        "f08fcadb1cb7422d89c102d2bac851fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0993d60ecc349a4900fcc44ba3edbf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab414bdb7683473a8a5f2a1c7046345b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff51d29ae79b499a9e2d144bb0692651": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c336b5810bb4e708748d155be282e90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7686da2d71954fe78ce40e390da5dbfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16ee83fd62e3474787f03c2849297db8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa08c359f40048f880be4070c1c4c092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c09d162e509b4f28b273e531253966ae",
              "IPY_MODEL_32b63c271c1b4fb7b02662ac939bb47e",
              "IPY_MODEL_e7430a6a4f564418b6e7e5755e6863f9"
            ],
            "layout": "IPY_MODEL_1620a74d6e5645ecb74a556cbad62905"
          }
        },
        "c09d162e509b4f28b273e531253966ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4efcc029f16a4a6095f40df6047eccd0",
            "placeholder": "",
            "style": "IPY_MODEL_8e8ae0d336d846418cf6b39e864ea91c",
            "value": "vocab.json:"
          }
        },
        "32b63c271c1b4fb7b02662ac939bb47e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecb94cbc51d24d3dbf753dbd6ac809aa",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0de5f8f1be1482fb3b6d57d64e5cdf0",
            "value": 1
          }
        },
        "e7430a6a4f564418b6e7e5755e6863f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d84f4345b6cf4b3d8f8fd8894f73108b",
            "placeholder": "",
            "style": "IPY_MODEL_2a085bd9ccf94a6f935eeff845a02cee",
            "value": "899k/?[00:00&lt;00:00,15.5MB/s]"
          }
        },
        "1620a74d6e5645ecb74a556cbad62905": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4efcc029f16a4a6095f40df6047eccd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e8ae0d336d846418cf6b39e864ea91c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecb94cbc51d24d3dbf753dbd6ac809aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b0de5f8f1be1482fb3b6d57d64e5cdf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d84f4345b6cf4b3d8f8fd8894f73108b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a085bd9ccf94a6f935eeff845a02cee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb2754e1e74240c6941f07118f054b2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab5e5b06e87b4208af43a120f6a1170a",
              "IPY_MODEL_f2ce217c76df4a17999fafb037538377",
              "IPY_MODEL_cdab124ddb8a4e528ab7f82da085334c"
            ],
            "layout": "IPY_MODEL_4f6d2b8fb70049cc8e34be975c8a851e"
          }
        },
        "ab5e5b06e87b4208af43a120f6a1170a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_014c875a34cc42fdbcf45b66cca96d79",
            "placeholder": "",
            "style": "IPY_MODEL_3ecf38dde2e94b659b8ab86e015df7bd",
            "value": "model.safetensors:100%"
          }
        },
        "f2ce217c76df4a17999fafb037538377": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5976ac8287ea43f788182b933514c8d6",
            "max": 500982668,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8cf836afcbee4513875dfaeb098919b5",
            "value": 500982668
          }
        },
        "cdab124ddb8a4e528ab7f82da085334c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb832d61a9144f639fd2764c63b4bb70",
            "placeholder": "",
            "style": "IPY_MODEL_0c3a75be0255445996e35c3c30670935",
            "value": "501M/501M[00:01&lt;00:00,345MB/s]"
          }
        },
        "4f6d2b8fb70049cc8e34be975c8a851e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "014c875a34cc42fdbcf45b66cca96d79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ecf38dde2e94b659b8ab86e015df7bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5976ac8287ea43f788182b933514c8d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cf836afcbee4513875dfaeb098919b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb832d61a9144f639fd2764c63b4bb70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c3a75be0255445996e35c3c30670935": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7b92b3d6b634114b5ffe8cbf4c0b56d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f718d9da5614e60ba280534dd2346df",
              "IPY_MODEL_b34dba7f59ac45689181a735137ec982",
              "IPY_MODEL_ad4cb70a10234ee4aeab78c2d8b9c204"
            ],
            "layout": "IPY_MODEL_a340f61d8f6e48b78c45b02bfbfb0dfd"
          }
        },
        "6f718d9da5614e60ba280534dd2346df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e4b63cb3f754641a5dcec3857635687",
            "placeholder": "",
            "style": "IPY_MODEL_eadf599c6f5e4dc3b56436a414b3f400",
            "value": "merges.txt:"
          }
        },
        "b34dba7f59ac45689181a735137ec982": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4ea77aaccc74b629c4e7ab8ebe13dcd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_069d073cc86649888c72e0fb06b13724",
            "value": 1
          }
        },
        "ad4cb70a10234ee4aeab78c2d8b9c204": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1760032586794ce6ba155a845f8996b6",
            "placeholder": "",
            "style": "IPY_MODEL_70dae7891adc43f584e78873173eeac3",
            "value": "456k/?[00:00&lt;00:00,33.4MB/s]"
          }
        },
        "a340f61d8f6e48b78c45b02bfbfb0dfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e4b63cb3f754641a5dcec3857635687": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eadf599c6f5e4dc3b56436a414b3f400": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4ea77aaccc74b629c4e7ab8ebe13dcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "069d073cc86649888c72e0fb06b13724": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1760032586794ce6ba155a845f8996b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70dae7891adc43f584e78873173eeac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ff81d4ebeb84ec992e10aa4c7a5f76f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92afddbcefa849e6897395328e820792",
              "IPY_MODEL_418243db23804f52bd98fa431c0593c7",
              "IPY_MODEL_2bd90de0efc54f5689d137f2861d5213"
            ],
            "layout": "IPY_MODEL_a56a560ce76740d6977b1b2fcbfeef19"
          }
        },
        "92afddbcefa849e6897395328e820792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10c585d3f0bd46ff8f5dcdeac87127b6",
            "placeholder": "",
            "style": "IPY_MODEL_0c36920b1509414989f7da60b57a2961",
            "value": "special_tokens_map.json:100%"
          }
        },
        "418243db23804f52bd98fa431c0593c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e103c36074d49358e6f215a2b885e10",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3557fb51f7c240e78431cfba5f53cac0",
            "value": 239
          }
        },
        "2bd90de0efc54f5689d137f2861d5213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77898240558b41f6965e2714e5f2607f",
            "placeholder": "",
            "style": "IPY_MODEL_6016029891d64ade9589f3c67b06c171",
            "value": "239/239[00:00&lt;00:00,22.4kB/s]"
          }
        },
        "a56a560ce76740d6977b1b2fcbfeef19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10c585d3f0bd46ff8f5dcdeac87127b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c36920b1509414989f7da60b57a2961": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e103c36074d49358e6f215a2b885e10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3557fb51f7c240e78431cfba5f53cac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77898240558b41f6965e2714e5f2607f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6016029891d64ade9589f3c67b06c171": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}